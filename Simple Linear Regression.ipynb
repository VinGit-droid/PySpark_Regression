{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ca6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf9c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92aa44",
   "metadata": {},
   "source": [
    "# Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61303fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/12 19:27:02 WARN Utils: Your hostname, myThinkPad resolves to a loopback address: 127.0.1.1; using 192.168.29.222 instead (on interface wlp3s0)\n",
      "23/07/12 19:27:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/12 19:27:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/12 19:27:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/07/12 19:27:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/07/12 19:27:04 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/07/12 19:27:04 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .appName(\"MLLib\")\n",
    "                     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a629e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1681eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Read the raw csv--------\n",
      "+-------------+------+\n",
      "|Time_to_Study|Grades|\n",
      "+-------------+------+\n",
      "|1            |1.5   |\n",
      "|5            |2.7   |\n",
      "|7            |3.1   |\n",
      "|3            |2.1   |\n",
      "|2            |1.8   |\n",
      "+-------------+------+\n",
      "\n",
      "root\n",
      " |-- Time_to_Study: integer (nullable = true)\n",
      " |-- Grades: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'Data_Science_Bootcamp/Regression_Algorithms/Simple_Linear_Regression/Student_Grades_Data.csv'\n",
    "\n",
    "\n",
    "# Read in the student data\n",
    "df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "print('-----Read the raw csv--------')\n",
    "df.limit(5).show(truncate=False)\n",
    "df.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52969388",
   "metadata": {},
   "source": [
    "# Create a Feature array by omitting the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78457e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time_to_Study']\n",
      "VectorAssembler_32038c50123c\n",
      "***********************************************\n",
      "+-------------+------+--------+\n",
      "|Time_to_Study|Grades|features|\n",
      "+-------------+------+--------+\n",
      "|            1|   1.5|   [1.0]|\n",
      "|            5|   2.7|   [5.0]|\n",
      "|            7|   3.1|   [7.0]|\n",
      "|            3|   2.1|   [3.0]|\n",
      "|            2|   1.8|   [2.0]|\n",
      "|            9|   3.9|   [9.0]|\n",
      "|            6|   2.9|   [6.0]|\n",
      "|           12|   4.5|  [12.0]|\n",
      "|           11|   4.3|  [11.0]|\n",
      "|            2|   1.8|   [2.0]|\n",
      "|            4|   2.4|   [4.0]|\n",
      "|            8|   3.5|   [8.0]|\n",
      "|           13|   4.8|  [13.0]|\n",
      "|            9|   3.9|   [9.0]|\n",
      "|           14|   5.0|  [14.0]|\n",
      "|           10|   4.1|  [10.0]|\n",
      "|            6|   2.9|   [6.0]|\n",
      "|           12|   4.5|  [12.0]|\n",
      "|            1|   1.5|   [1.0]|\n",
      "|            4|   2.4|   [4.0]|\n",
      "+-------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_cols = df.columns[:-1] \n",
    "print(feature_cols)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vect_assembler = VectorAssembler(inputCols=feature_cols,outputCol=\"features\")\n",
    "print(vect_assembler)\n",
    "print('***********************************************')\n",
    "#Utilize Assembler created above in order to add the feature column\n",
    "data_w_features = vect_assembler.transform(df)\n",
    "data_w_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1be7c8",
   "metadata": {},
   "source": [
    "Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f39588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+--------+\n",
      "|Time_to_Study|Grades|features|\n",
      "+-------------+------+--------+\n",
      "|            1|   1.5|   [1.0]|\n",
      "|            5|   2.7|   [5.0]|\n",
      "|            7|   3.1|   [7.0]|\n",
      "|            3|   2.1|   [3.0]|\n",
      "|            2|   1.8|   [2.0]|\n",
      "+-------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Display the data having additional column named features. Had it been multiple linear regression problem, \n",
    "## you could see all the independent variable values combined in one list\n",
    "data_w_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1265b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|features|Grades|\n",
      "+--------+------+\n",
      "|   [1.0]|   1.5|\n",
      "|   [5.0]|   2.7|\n",
      "|   [7.0]|   3.1|\n",
      "|   [3.0]|   2.1|\n",
      "|   [2.0]|   1.8|\n",
      "|   [9.0]|   3.9|\n",
      "|   [6.0]|   2.9|\n",
      "|  [12.0]|   4.5|\n",
      "|  [11.0]|   4.3|\n",
      "|   [2.0]|   1.8|\n",
      "|   [4.0]|   2.4|\n",
      "|   [8.0]|   3.5|\n",
      "|  [13.0]|   4.8|\n",
      "|   [9.0]|   3.9|\n",
      "|  [14.0]|   5.0|\n",
      "|  [10.0]|   4.1|\n",
      "|   [6.0]|   2.9|\n",
      "|  [12.0]|   4.5|\n",
      "|   [1.0]|   1.5|\n",
      "|   [4.0]|   2.4|\n",
      "+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select only Features and Label from previous dataset as we need these two entities for building machine learning model\n",
    "\n",
    "finalized_data = data_w_features.select(\"features\",\"Grades\")\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9d575",
   "metadata": {},
   "source": [
    "# Split the data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e38f651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            Grades|\n",
      "+-------+------------------+\n",
      "|  count|                40|\n",
      "|   mean|            3.2875|\n",
      "| stddev|1.1216259946144578|\n",
      "|    min|               1.5|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|            Grades|\n",
      "+-------+------------------+\n",
      "|  count|                10|\n",
      "|   mean|              2.96|\n",
      "| stddev|1.0479609831583532|\n",
      "|    min|               1.8|\n",
      "|    max|               4.8|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split the data into training and test model with 70% obs. going in training and 30% in testing\n",
    "train_dataset, test_dataset = finalized_data.randomSplit([0.8, 0.2])\n",
    "train_dataset.describe().show()\n",
    "test_dataset.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a577b1",
   "metadata": {},
   "source": [
    "# Perform Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e422ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/10 10:30:04 WARN Instrumentation: [69e2434a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "+--------+------+------------------+\n",
      "|features|Grades|        prediction|\n",
      "+--------+------+------------------+\n",
      "|   [2.0]|   1.8| 1.828686653477401|\n",
      "|   [2.0]|   1.8| 1.828686653477401|\n",
      "|   [3.0]|   2.1| 2.101362045350784|\n",
      "|   [4.0]|   2.4| 2.374037437224167|\n",
      "|   [5.0]|   2.7|  2.64671282909755|\n",
      "|   [6.0]|   2.9| 2.919388220970933|\n",
      "|   [7.0]|   3.1|3.1920636128443163|\n",
      "|   [8.0]|   3.5|3.4647390047176994|\n",
      "|  [12.0]|   4.5| 4.555440572211232|\n",
      "|  [13.0]|   4.8| 4.828115964084615|\n",
      "+--------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import Linear Regression class called LinearRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#Create the Linear Regression object named having feature column as features and Label column as Time_to_Study\n",
    "LinReg = LinearRegression(featuresCol=\"features\", labelCol=\"Grades\")\n",
    "\n",
    "#Train the model on the training using fit() method.\n",
    "model = LinReg.fit(train_dataset)\n",
    "\n",
    "#Predict the Grades using the evulate method\n",
    "pred = model.evaluate(test_dataset)\n",
    "\n",
    "#Show the predicted Grade values along side actual Grade values\n",
    "pred.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b72bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6885422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficient of the model is : DenseVector([0.2727])\n",
      "The Intercept of the model is : 1.283336\n",
      "RegressionEvaluator_6d1fd336fcfa\n",
      "RMSE: 0.044\n",
      "MSE: 0.002\n",
      "MAE: 0.037\n",
      "r2: 0.998\n"
     ]
    }
   ],
   "source": [
    "#Find out coefficient value\n",
    "coefficient = model.coefficients\n",
    "print (\"The coefficient of the model is : %a\" %coefficient)\n",
    "\n",
    "#Find out intercept Value\n",
    "intercept = model.intercept\n",
    "print (\"The Intercept of the model is : %f\" %intercept)\n",
    "\n",
    "#Evaluate the model using metric like Mean Absolute Error(MAE), Root Mean Square Error(RMSE) and R-Square\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluation = RegressionEvaluator(labelCol=\"Grades\", predictionCol=\"prediction\")\n",
    "print(evaluation)\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = evaluation.evaluate(pred.predictions, {evaluation.metricName: \"rmse\"})\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = evaluation.evaluate(pred.predictions, {evaluation.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = evaluation.evaluate(pred.predictions, {evaluation.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# r2 - coefficient of determination\n",
    "r2 = evaluation.evaluate(pred.predictions, {evaluation.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" %r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be3e917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|   [2.0]|\n",
      "|   [2.0]|\n",
      "|   [3.0]|\n",
      "|   [4.0]|\n",
      "|   [5.0]|\n",
      "|   [6.0]|\n",
      "|   [7.0]|\n",
      "|   [8.0]|\n",
      "|  [12.0]|\n",
      "|  [13.0]|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create Unlabeled dataset  to contain only feature column\n",
    "unlabeled_dataset = test_dataset.select('features')\n",
    "\n",
    "#Display the content of unlabeled_dataset\n",
    "unlabeled_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "790f1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the model output for fresh & unseen test data using transform() method\n",
    "new_predictions = model.transform(unlabeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b2c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|features|        prediction|\n",
      "+--------+------------------+\n",
      "|   [2.0]| 1.828686653477401|\n",
      "|   [2.0]| 1.828686653477401|\n",
      "|   [3.0]| 2.101362045350784|\n",
      "|   [4.0]| 2.374037437224167|\n",
      "|   [5.0]|  2.64671282909755|\n",
      "|   [6.0]| 2.919388220970933|\n",
      "|   [7.0]|3.1920636128443163|\n",
      "|   [8.0]|3.4647390047176994|\n",
      "|  [12.0]| 4.555440572211232|\n",
      "|  [13.0]| 4.828115964084615|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the new prediction values\n",
    "new_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c998f1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Predict_Grade \u001b[38;5;241m=\u001b[39m \u001b[43mLinReg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction\u001b[49m(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      2\u001b[0m help(LinearRegression)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'prediction'"
     ]
    }
   ],
   "source": [
    "Predict_Grade = LinReg.prediction(10)\n",
    "help(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a02fd5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LinearRegression in module pyspark.ml.regression object:\n",
      "\n",
      "class LinearRegression(_JavaRegressor, _LinearRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n",
      " |  LinearRegression(*, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0)\n",
      " |  \n",
      " |  Linear regression.\n",
      " |  \n",
      " |  The learning objective is to minimize the specified loss function, with regularization.\n",
      " |  This supports two kinds of loss:\n",
      " |  \n",
      " |  * squaredError (a.k.a squared loss)\n",
      " |  * huber (a hybrid of squared error for relatively small errors and absolute error for     relatively large ones, and we estimate the scale parameter from training data)\n",
      " |  \n",
      " |  This supports multiple types of regularization:\n",
      " |  \n",
      " |  * none (a.k.a. ordinary least squares)\n",
      " |  * L2 (ridge regression)\n",
      " |  * L1 (Lasso)\n",
      " |  * L2 + L1 (elastic net)\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Fitting with huber loss only supports none and L2 regularization.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from pyspark.ml.linalg import Vectors\n",
      " |  >>> df = spark.createDataFrame([\n",
      " |  ...     (1.0, 2.0, Vectors.dense(1.0)),\n",
      " |  ...     (0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n",
      " |  >>> lr = LinearRegression(regParam=0.0, solver=\"normal\", weightCol=\"weight\")\n",
      " |  >>> lr.setMaxIter(5)\n",
      " |  LinearRegression...\n",
      " |  >>> lr.getMaxIter()\n",
      " |  5\n",
      " |  >>> lr.setRegParam(0.1)\n",
      " |  LinearRegression...\n",
      " |  >>> lr.getRegParam()\n",
      " |  0.1\n",
      " |  >>> lr.setRegParam(0.0)\n",
      " |  LinearRegression...\n",
      " |  >>> model = lr.fit(df)\n",
      " |  >>> model.setFeaturesCol(\"features\")\n",
      " |  LinearRegressionModel...\n",
      " |  >>> model.setPredictionCol(\"newPrediction\")\n",
      " |  LinearRegressionModel...\n",
      " |  >>> model.getMaxIter()\n",
      " |  5\n",
      " |  >>> model.getMaxBlockSizeInMB()\n",
      " |  0.0\n",
      " |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n",
      " |  >>> abs(model.predict(test0.head().features) - (-1.0)) < 0.001\n",
      " |  True\n",
      " |  >>> abs(model.transform(test0).head().newPrediction - (-1.0)) < 0.001\n",
      " |  True\n",
      " |  >>> abs(model.coefficients[0] - 1.0) < 0.001\n",
      " |  True\n",
      " |  >>> abs(model.intercept - 0.0) < 0.001\n",
      " |  True\n",
      " |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
      " |  >>> abs(model.transform(test1).head().newPrediction - 1.0) < 0.001\n",
      " |  True\n",
      " |  >>> lr.setParams(featuresCol=\"vector\")\n",
      " |  LinearRegression...\n",
      " |  >>> lr_path = temp_path + \"/lr\"\n",
      " |  >>> lr.save(lr_path)\n",
      " |  >>> lr2 = LinearRegression.load(lr_path)\n",
      " |  >>> lr2.getMaxIter()\n",
      " |  5\n",
      " |  >>> model_path = temp_path + \"/lr_model\"\n",
      " |  >>> model.save(model_path)\n",
      " |  >>> model2 = LinearRegressionModel.load(model_path)\n",
      " |  >>> model.coefficients[0] == model2.coefficients[0]\n",
      " |  True\n",
      " |  >>> model.intercept == model2.intercept\n",
      " |  True\n",
      " |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
      " |  True\n",
      " |  >>> model.numFeatures\n",
      " |  1\n",
      " |  >>> model.write().format(\"pmml\").save(model_path + \"_2\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      _JavaRegressor\n",
      " |      Regressor\n",
      " |      pyspark.ml.wrapper.JavaPredictor\n",
      " |      pyspark.ml.base.Predictor\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      _LinearRegressionParams\n",
      " |      pyspark.ml.base._PredictorParams\n",
      " |      pyspark.ml.param.shared.HasLabelCol\n",
      " |      pyspark.ml.param.shared.HasFeaturesCol\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      pyspark.ml.param.shared.HasRegParam\n",
      " |      pyspark.ml.param.shared.HasElasticNetParam\n",
      " |      pyspark.ml.param.shared.HasMaxIter\n",
      " |      pyspark.ml.param.shared.HasTol\n",
      " |      pyspark.ml.param.shared.HasFitIntercept\n",
      " |      pyspark.ml.param.shared.HasStandardization\n",
      " |      pyspark.ml.param.shared.HasWeightCol\n",
      " |      pyspark.ml.param.shared.HasSolver\n",
      " |      pyspark.ml.param.shared.HasAggregationDepth\n",
      " |      pyspark.ml.param.shared.HasLoss\n",
      " |      pyspark.ml.param.shared.HasMaxBlockSizeInMB\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0)\n",
      " |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                  standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                  loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n",
      " |  \n",
      " |  setAggregationDepth(self, value: int) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`aggregationDepth`.\n",
      " |  \n",
      " |  setElasticNetParam(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`elasticNetParam`.\n",
      " |  \n",
      " |  setEpsilon(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`epsilon`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  setFitIntercept(self, value: bool) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`fitIntercept`.\n",
      " |  \n",
      " |  setLoss(self, value: str) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`loss`.\n",
      " |  \n",
      " |  setMaxBlockSizeInMB(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`maxBlockSizeInMB`.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |  \n",
      " |  setMaxIter(self, value: int) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`maxIter`.\n",
      " |  \n",
      " |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0) -> 'LinearRegression'\n",
      " |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                   standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                   loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n",
      " |      Sets params for linear regression.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setRegParam(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`regParam`.\n",
      " |  \n",
      " |  setSolver(self, value: str) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`solver`.\n",
      " |  \n",
      " |  setStandardization(self, value: bool) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`standardization`.\n",
      " |  \n",
      " |  setTol(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`tol`.\n",
      " |  \n",
      " |  setWeightCol(self, value: str) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`weightCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n",
      " |  \n",
      " |  __orig_bases__ = (pyspark.ml.regression._JavaRegressor[ForwardRef('Lin...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Predictor:\n",
      " |  \n",
      " |  setFeaturesCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`featuresCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setLabelCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`labelCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setPredictionCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param: pyspark.ml.param.Param) -> None\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      params : dict or list or tuple, optional\n",
      " |          an optional param map that overrides embedded params. If a list/tuple of\n",
      " |          param maps is given, this calls fit on each param map and returns a list of\n",
      " |          models.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n",
      " |          fitted model(s)\n",
      " |  \n",
      " |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      paramMaps : :py:class:`collections.abc.Sequence`\n",
      " |          A Sequence of param maps.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`_FitMultipleIterator`\n",
      " |          A thread safe iterable which contains one model for each param map. Each\n",
      " |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |          using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _LinearRegressionParams:\n",
      " |  \n",
      " |  getEpsilon(self) -> float\n",
      " |      Gets the value of epsilon or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _LinearRegressionParams:\n",
      " |  \n",
      " |  epsilon = Param(parent='undefined', name='epsilon', doc='T...s. Must b...\n",
      " |  \n",
      " |  loss = Param(parent='undefined', name='loss', doc='The ...imized. Supp...\n",
      " |  \n",
      " |  solver = Param(parent='undefined', name='solver', doc='Th...ation. Sup...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  getLabelCol(self) -> str\n",
      " |      Gets the value of labelCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  getFeaturesCol(self) -> str\n",
      " |      Gets the value of featuresCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self) -> str\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n",
      " |  \n",
      " |  getRegParam(self) -> float\n",
      " |      Gets the value of regParam or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n",
      " |  \n",
      " |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasElasticNetParam:\n",
      " |  \n",
      " |  getElasticNetParam(self) -> float\n",
      " |      Gets the value of elasticNetParam or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasElasticNetParam:\n",
      " |  \n",
      " |  elasticNetParam = Param(parent='undefined', name='elasticNetParam'...L...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  getMaxIter(self) -> int\n",
      " |      Gets the value of maxIter or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasTol:\n",
      " |  \n",
      " |  getTol(self) -> float\n",
      " |      Gets the value of tol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n",
      " |  \n",
      " |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n",
      " |  \n",
      " |  getFitIntercept(self) -> bool\n",
      " |      Gets the value of fitIntercept or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n",
      " |  \n",
      " |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasStandardization:\n",
      " |  \n",
      " |  getStandardization(self) -> bool\n",
      " |      Gets the value of standardization or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasStandardization:\n",
      " |  \n",
      " |  standardization = Param(parent='undefined', name='standardization'...t...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  getWeightCol(self) -> str\n",
      " |      Gets the value of weightCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasSolver:\n",
      " |  \n",
      " |  getSolver(self) -> str\n",
      " |      Gets the value of solver or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasAggregationDepth:\n",
      " |  \n",
      " |  getAggregationDepth(self) -> int\n",
      " |      Gets the value of aggregationDepth or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasAggregationDepth:\n",
      " |  \n",
      " |  aggregationDepth = Param(parent='undefined', name='aggregationDepth', ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLoss:\n",
      " |  \n",
      " |  getLoss(self) -> str\n",
      " |      Gets the value of loss or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n",
      " |  \n",
      " |  getMaxBlockSizeInMB(self) -> float\n",
      " |      Gets the value of maxBlockSizeInMB or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n",
      " |  \n",
      " |  maxBlockSizeInMB = Param(parent='undefined', name='maxBlockSizeInMB......\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self) -> str\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName: str) -> bool\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self) -> pyspark.ml.util.JavaMLWriter\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path: str) -> None\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path: str) -> ~RL from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LinReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a827f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94d298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
