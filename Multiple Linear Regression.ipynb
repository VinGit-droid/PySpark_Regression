{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d44581",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf9c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11fc96b",
   "metadata": {},
   "source": [
    "# Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87eb11f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/10 10:33:01 WARN Utils: Your hostname, myThinkPad resolves to a loopback address: 127.0.1.1; using 192.168.163.222 instead (on interface wlp3s0)\n",
      "23/07/10 10:33:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/10 10:33:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/10 10:33:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .appName(\"MLLib\")\n",
    "                     .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d352e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=MLLib, master=local[2]) created by getOrCreate at /tmp/ipykernel_291642/3522750921.py:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext \n\u001b[0;32m----> 2\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(sc)\n",
      "File \u001b[0;32m/media/vin/v_drive/2023/PySpark_Software/spark-3.3.2-bin-hadoop3-scala2.13/python/pyspark/context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[0;32m--> 195\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    198\u001b[0m         master,\n\u001b[1;32m    199\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    209\u001b[0m     )\n",
      "File \u001b[0;32m/media/vin/v_drive/2023/PySpark_Software/spark-3.3.2-bin-hadoop3-scala2.13/python/pyspark/context.py:430\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    427\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    435\u001b[0m             currentAppName,\n\u001b[1;32m    436\u001b[0m             currentMaster,\n\u001b[1;32m    437\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    438\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    439\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    440\u001b[0m         )\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=MLLib, master=local[2]) created by getOrCreate at /tmp/ipykernel_291642/3522750921.py:3 "
     ]
    }
   ],
   "source": [
    "  from pyspark import SparkContext \n",
    "  sc = SparkContext()\n",
    "  print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f583ee31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Read the raw Restaurant csv data--------\n",
      "root\n",
      " |-- Miscellaneous_Expenses: double (nullable = true)\n",
      " |-- Food_Innovation_Spend: double (nullable = true)\n",
      " |-- Advertising: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n",
      "+----------------------+---------------------+-----------+-------+---------+\n",
      "|Miscellaneous_Expenses|Food_Innovation_Spend|Advertising|City   |Profit   |\n",
      "+----------------------+---------------------+-----------+-------+---------+\n",
      "|138671.8              |167497.2             |475918.1   |Chicago|202443.83|\n",
      "|153151.59             |164745.7             |448032.53  |Mumbai |201974.06|\n",
      "|102919.55             |155589.51            |412068.54  |Tokyo  |201232.39|\n",
      "|120445.85             |146520.41            |387333.62  |Chicago|193083.99|\n",
      "|93165.77              |144255.34            |370302.42  |Tokyo  |176369.94|\n",
      "+----------------------+---------------------+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = 'Data_Science_Bootcamp/Regression_Algorithms/Multiple_Linear_Regression/Restaurant_Profit_Data.csv'\n",
    "\n",
    "\n",
    "# Read in the student data\n",
    "df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "print('-----Read the raw Restaurant csv data--------')\n",
    "df.printSchema()\n",
    "df.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1700903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['City']\n",
      "['Miscellaneous_Expenses', 'Food_Innovation_Spend', 'Advertising']\n",
      "----------------------------\n",
      "1  categorical features\n",
      "3  numerical features\n"
     ]
    }
   ],
   "source": [
    "#Create features storing categorical & numerical variables, omitting the last column\n",
    "categorical_cols = [item[0] for item in df.dtypes if item[1].startswith('string')]\n",
    "print(categorical_cols)\n",
    "\n",
    "numerical_cols = [item[0] for item in df.dtypes if item[1].startswith('int') | item[1].startswith('double')][:-1]\n",
    "print(numerical_cols)\n",
    "\n",
    "print(\"----------------------------\")\n",
    "#Print number of categorical as well as numerical features.\n",
    "print(str(len(categorical_cols)) + '  categorical features')\n",
    "print(str(len(numerical_cols)) + '  numerical features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6c989a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nindexer = StringIndexer(inputCol=\"City\", outputCol=\"City_Dummy\")\\ndf_new = indexer.fit(df).transform(df)\\ndf_new.show()\\ndf_new = df_new.drop(\"City\")\\ndf_new.printSchema()\\ndf_new.show()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert categorical column City to numerical\n",
    "'''\n",
    "indexer = StringIndexer(inputCol=\"City\", outputCol=\"City_Dummy\")\n",
    "df_new = indexer.fit(df).transform(df)\n",
    "df_new.show()\n",
    "df_new = df_new.drop(\"City\")\n",
    "df_new.printSchema()\n",
    "df_new.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969ee97",
   "metadata": {},
   "source": [
    "# Example to convert categorical column to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9467b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       c|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/10 10:33:49 WARN StringIndexerModel: Input column category does not exist during transformation. Skip StringIndexerModel for this column.\n",
      "+----------------------+---------------------+-----------+-------+---------+\n",
      "|Miscellaneous_Expenses|Food_Innovation_Spend|Advertising|   City|   Profit|\n",
      "+----------------------+---------------------+-----------+-------+---------+\n",
      "|              138671.8|             167497.2|   475918.1|Chicago|202443.83|\n",
      "|             153151.59|             164745.7|  448032.53| Mumbai|201974.06|\n",
      "|             102919.55|            155589.51|  412068.54|  Tokyo|201232.39|\n",
      "|             120445.85|            146520.41|  387333.62|Chicago|193083.99|\n",
      "|              93165.77|            144255.34|  370302.42|  Tokyo|176369.94|\n",
      "|             101588.71|             134024.9|  366995.36|Chicago|167173.12|\n",
      "|             148972.87|            136763.46|  131850.82| Mumbai|166304.51|\n",
      "|             147304.06|            132446.13|  328010.68|  Tokyo| 165934.6|\n",
      "|             150492.95|            122690.52|  315747.29|Chicago|162393.77|\n",
      "|             110453.17|            125482.88|  309115.62| Mumbai|159941.96|\n",
      "|             112368.11|            104061.08|  233294.95|  Tokyo|156303.95|\n",
      "|              93564.61|            102819.96|  253878.55| Mumbai| 154441.4|\n",
      "|             129094.38|             96011.75|  253973.44|  Tokyo|151767.52|\n",
      "|             137269.07|             94140.39|  256798.93| Mumbai|144489.35|\n",
      "|             158321.42|            122091.24|  260646.92|  Tokyo|142784.65|\n",
      "|             124390.84|            116671.61|  265910.23|Chicago|140099.04|\n",
      "|             123371.55|             80161.11|  268480.06| Mumbai|137174.93|\n",
      "|             146851.58|             96805.16|  286708.31|Chicago|135552.37|\n",
      "|             115949.79|             93897.16|  299053.57|  Tokyo| 134448.9|\n",
      "|             155288.11|              88567.7|     4134.0|Chicago|132958.86|\n",
      "+----------------------+---------------------+-----------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "testdf = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "testdf.show()\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(testdf).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177d98e7",
   "metadata": {},
   "source": [
    "## First using StringIndexer to convert string/text values into numerical values followed by OneHotEncoderEstimator \n",
    "## Spark MLLibto convert each Stringindexed or transformed values into One Hot Encoded values.\n",
    "## VectorAssembler is being used to assemble all the features into one vector from multiple columns that contain type double \n",
    "## Also appending every step of the process in a stages array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a207f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexer_808613d60161\n",
      "OneHotEncoder_4e2c8f80e2d0\n",
      "[StringIndexer_808613d60161, OneHotEncoder_4e2c8f80e2d0]\n",
      "['City_catVec', 'Miscellaneous_Expenses', 'Food_Innovation_Spend', 'Advertising']\n",
      "VectorAssembler_ef1c69f55352\n",
      "[StringIndexer_808613d60161, OneHotEncoder_4e2c8f80e2d0, VectorAssembler_ef1c69f55352]\n"
     ]
    }
   ],
   "source": [
    "# First using StringIndexer to convert string/text values into numerical values followed by OneHotEncoderEstimator \n",
    "# Spark MLLibto convert each Stringindexed or transformed values into One Hot Encoded values.\n",
    "# VectorAssembler is being used to assemble all the features into one vector from multiple columns that contain type double \n",
    "# Also appending every step of the process in a stages array\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "stages = []\n",
    "for categoricalCol in categorical_cols:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    OHencoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"_catVec\"])\n",
    "\n",
    "print(stringIndexer)\n",
    "print(OHencoder)\n",
    "stages += [stringIndexer, OHencoder]\n",
    "print(stages)\n",
    "assemblerInputs = [c + \"_catVec\" for c in categorical_cols] + numerical_cols\n",
    "print(assemblerInputs)\n",
    "Vectassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "print(Vectassembler)\n",
    "stages += [Vectassembler]\n",
    "print(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0482bf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7e2387f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>Miscellaneous_Expenses</th>\n",
       "      <th>Food_Innovation_Spend</th>\n",
       "      <th>Advertising</th>\n",
       "      <th>City</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0, 0.0, 138671.8, 167497.2, 475918.1]</td>\n",
       "      <td>138671.80</td>\n",
       "      <td>167497.20</td>\n",
       "      <td>475918.10</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>202443.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 1.0, 153151.59, 164745.7, 448032.53]</td>\n",
       "      <td>153151.59</td>\n",
       "      <td>164745.70</td>\n",
       "      <td>448032.53</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>201974.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 102919.55, 155589.51, 412068.54]</td>\n",
       "      <td>102919.55</td>\n",
       "      <td>155589.51</td>\n",
       "      <td>412068.54</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>201232.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0, 0.0, 120445.85, 146520.41, 387333.62]</td>\n",
       "      <td>120445.85</td>\n",
       "      <td>146520.41</td>\n",
       "      <td>387333.62</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>193083.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 93165.77, 144255.34, 370302.42]</td>\n",
       "      <td>93165.77</td>\n",
       "      <td>144255.34</td>\n",
       "      <td>370302.42</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>176369.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      features  Miscellaneous_Expenses   \n",
       "0     [1.0, 0.0, 138671.8, 167497.2, 475918.1]               138671.80  \\\n",
       "1   [0.0, 1.0, 153151.59, 164745.7, 448032.53]               153151.59   \n",
       "2  [0.0, 0.0, 102919.55, 155589.51, 412068.54]               102919.55   \n",
       "3  [1.0, 0.0, 120445.85, 146520.41, 387333.62]               120445.85   \n",
       "4   [0.0, 0.0, 93165.77, 144255.34, 370302.42]                93165.77   \n",
       "\n",
       "   Food_Innovation_Spend  Advertising     City     Profit  \n",
       "0              167497.20    475918.10  Chicago  202443.83  \n",
       "1              164745.70    448032.53   Mumbai  201974.06  \n",
       "2              155589.51    412068.54    Tokyo  201232.39  \n",
       "3              146520.41    387333.62  Chicago  193083.99  \n",
       "4              144255.34    370302.42    Tokyo  176369.94  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a Spark MLLib pipeline to apply all the stages of transformation\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "cols = df.columns\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "data = pipelineModel.transform(df)\n",
    "selectedCols = ['features']+cols\n",
    "data = data.select(selectedCols)\n",
    "pd.DataFrame(data.take(5), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d36c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+---------------------+-----------+-------+---------+\n",
      "|            features|Miscellaneous_Expenses|Food_Innovation_Spend|Advertising|   City|   Profit|\n",
      "+--------------------+----------------------+---------------------+-----------+-------+---------+\n",
      "|[1.0,0.0,138671.8...|              138671.8|             167497.2|   475918.1|Chicago|202443.83|\n",
      "|[0.0,1.0,153151.5...|             153151.59|             164745.7|  448032.53| Mumbai|201974.06|\n",
      "|[0.0,0.0,102919.5...|             102919.55|            155589.51|  412068.54|  Tokyo|201232.39|\n",
      "|[1.0,0.0,120445.8...|             120445.85|            146520.41|  387333.62|Chicago|193083.99|\n",
      "|[0.0,0.0,93165.77...|              93165.77|            144255.34|  370302.42|  Tokyo|176369.94|\n",
      "|[1.0,0.0,101588.7...|             101588.71|             134024.9|  366995.36|Chicago|167173.12|\n",
      "|[0.0,1.0,148972.8...|             148972.87|            136763.46|  131850.82| Mumbai|166304.51|\n",
      "|[0.0,0.0,147304.0...|             147304.06|            132446.13|  328010.68|  Tokyo| 165934.6|\n",
      "|[1.0,0.0,150492.9...|             150492.95|            122690.52|  315747.29|Chicago|162393.77|\n",
      "|[0.0,1.0,110453.1...|             110453.17|            125482.88|  309115.62| Mumbai|159941.96|\n",
      "|[0.0,0.0,112368.1...|             112368.11|            104061.08|  233294.95|  Tokyo|156303.95|\n",
      "|[0.0,1.0,93564.61...|              93564.61|            102819.96|  253878.55| Mumbai| 154441.4|\n",
      "|[0.0,0.0,129094.3...|             129094.38|             96011.75|  253973.44|  Tokyo|151767.52|\n",
      "|[0.0,1.0,137269.0...|             137269.07|             94140.39|  256798.93| Mumbai|144489.35|\n",
      "|[0.0,0.0,158321.4...|             158321.42|            122091.24|  260646.92|  Tokyo|142784.65|\n",
      "|[1.0,0.0,124390.8...|             124390.84|            116671.61|  265910.23|Chicago|140099.04|\n",
      "|[0.0,1.0,123371.5...|             123371.55|             80161.11|  268480.06| Mumbai|137174.93|\n",
      "|[1.0,0.0,146851.5...|             146851.58|             96805.16|  286708.31|Chicago|135552.37|\n",
      "|[0.0,0.0,115949.7...|             115949.79|             93897.16|  299053.57|  Tokyo| 134448.9|\n",
      "|[1.0,0.0,155288.1...|             155288.11|              88567.7|     4134.0|Chicago|132958.86|\n",
      "+--------------------+----------------------+---------------------+-----------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display the data having additional column named features. Since it's a multiple linear regression problem, hence all the\n",
    "# independent variable values are shown as one vector\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab9bfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|   Profit|\n",
      "+--------------------+---------+\n",
      "|[1.0,0.0,138671.8...|202443.83|\n",
      "|[0.0,1.0,153151.5...|201974.06|\n",
      "|[0.0,0.0,102919.5...|201232.39|\n",
      "|[1.0,0.0,120445.8...|193083.99|\n",
      "|[0.0,0.0,93165.77...|176369.94|\n",
      "|[1.0,0.0,101588.7...|167173.12|\n",
      "|[0.0,1.0,148972.8...|166304.51|\n",
      "|[0.0,0.0,147304.0...| 165934.6|\n",
      "|[1.0,0.0,150492.9...|162393.77|\n",
      "|[0.0,1.0,110453.1...|159941.96|\n",
      "|[0.0,0.0,112368.1...|156303.95|\n",
      "|[0.0,1.0,93564.61...| 154441.4|\n",
      "|[0.0,0.0,129094.3...|151767.52|\n",
      "|[0.0,1.0,137269.0...|144489.35|\n",
      "|[0.0,0.0,158321.4...|142784.65|\n",
      "|[1.0,0.0,124390.8...|140099.04|\n",
      "|[0.0,1.0,123371.5...|137174.93|\n",
      "|[1.0,0.0,146851.5...|135552.37|\n",
      "|[0.0,0.0,115949.7...| 134448.9|\n",
      "|[1.0,0.0,155288.1...|132958.86|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select only Features and Label from previous dataset as we need these two entities for building machine learning model\n",
    "finalized_data = data.select(\"features\",\"Profit\")\n",
    "\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe42c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30265852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e51dcb4",
   "metadata": {},
   "source": [
    "# Create a Feature array by omitting the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "221fd293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|   Profit|\n",
      "+--------------------+---------+\n",
      "|[1.0,0.0,138671.8...|202443.83|\n",
      "|[0.0,1.0,153151.5...|201974.06|\n",
      "|[0.0,0.0,102919.5...|201232.39|\n",
      "|[1.0,0.0,120445.8...|193083.99|\n",
      "|[0.0,0.0,93165.77...|176369.94|\n",
      "|[1.0,0.0,101588.7...|167173.12|\n",
      "|[0.0,1.0,148972.8...|166304.51|\n",
      "|[0.0,0.0,147304.0...| 165934.6|\n",
      "|[1.0,0.0,150492.9...|162393.77|\n",
      "|[0.0,1.0,110453.1...|159941.96|\n",
      "|[0.0,0.0,112368.1...|156303.95|\n",
      "|[0.0,1.0,93564.61...| 154441.4|\n",
      "|[0.0,0.0,129094.3...|151767.52|\n",
      "|[0.0,1.0,137269.0...|144489.35|\n",
      "|[0.0,0.0,158321.4...|142784.65|\n",
      "|[1.0,0.0,124390.8...|140099.04|\n",
      "|[0.0,1.0,123371.5...|137174.93|\n",
      "|[1.0,0.0,146851.5...|135552.37|\n",
      "|[0.0,0.0,115949.7...| 134448.9|\n",
      "|[1.0,0.0,155288.1...|132958.86|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select only Features and Label from previous dataset as we need these two entities for building machine learning model\n",
    "\n",
    "finalized_data = data.select(\"features\",\"Profit\")\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14210be",
   "metadata": {},
   "source": [
    "# Split the data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c50ad5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            Profit|\n",
      "+-------+------------------+\n",
      "|  count|                37|\n",
      "|   mean|123397.53324324326|\n",
      "| stddev| 43396.33854490875|\n",
      "|    min|           24863.4|\n",
      "|    max|         202443.83|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|            Profit|\n",
      "+-------+------------------+\n",
      "|  count|                13|\n",
      "|   mean|118771.01769230771|\n",
      "| stddev| 31094.19644871723|\n",
      "|    min|          75382.33|\n",
      "|    max|         176369.94|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split the data into training and test model with 70% obs. going in training and 30% in testing\n",
    "train_dataset, test_dataset = finalized_data.randomSplit([0.8, 0.2])\n",
    "train_dataset.describe().show()\n",
    "test_dataset.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fe289",
   "metadata": {},
   "source": [
    "# Perform Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c976a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/07/10 10:34:17 WARN Instrumentation: [0c0d9568] regParam is zero, which might cause numerical instability and overfitting.\n",
      "+--------------------+---------+------------------+\n",
      "|            features|   Profit|        prediction|\n",
      "+--------------------+---------+------------------+\n",
      "|[0.0,0.0,86484.77...| 87980.83| 82003.39136829594|\n",
      "|[0.0,0.0,93165.77...|176369.94|183248.08673250358|\n",
      "|[0.0,0.0,112368.1...|156303.95| 144984.7631362705|\n",
      "|[0.0,0.0,128830.2...|100890.19| 81123.05789257553|\n",
      "|[0.0,0.0,184419.5...|113464.38| 108059.5331207614|\n",
      "|[0.0,1.0,120320.0...| 88421.91| 82194.60993303913|\n",
      "|[0.0,1.0,156580.1...| 75382.33| 71437.49500920527|\n",
      "|[0.0,1.0,159467.9...| 106894.8| 95904.00873297648|\n",
      "|[1.0,0.0,86821.44...|106661.51|101531.38324942328|\n",
      "|[1.0,0.0,124390.8...|140099.04|158993.41618579935|\n",
      "|[1.0,0.0,150492.9...|162393.77|163764.43717146514|\n",
      "|[1.0,0.0,154475.9...|107665.56|108102.13724443913|\n",
      "|[1.0,0.0,155547.4...|121495.02|127350.73519832618|\n",
      "+--------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import Linear Regression class called LinearRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "#Create the Linear Regression object named having feature column as features and Label column as Time_to_Study\n",
    "LinReg = LinearRegression(featuresCol=\"features\", labelCol=\"Profit\")\n",
    "\n",
    "#Train the model on the training using fit() method.\n",
    "model = LinReg.fit(train_dataset)\n",
    "\n",
    "#Predict the Grades using the evulate method\n",
    "pred = model.evaluate(test_dataset)\n",
    "\n",
    "#Show the predicted Grade values along side actual Grade values\n",
    "pred.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938a9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "735d8c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficient of the model is : DenseVector([3226.9572, 1144.0305, -0.0552, 0.8511, 0.0219])\n",
      "The Intercept of the model is : 57525.071965\n",
      "RMSE: 9710.927\n",
      "MSE: 94302108.490\n",
      "MAE: 7861.318\n",
      "r2: 0.894\n"
     ]
    }
   ],
   "source": [
    "#Find out coefficient value\n",
    "coefficient = model.coefficients\n",
    "print (\"The coefficient of the model is : %a\" %coefficient)\n",
    "\n",
    "#Find out intercept Value\n",
    "intercept = model.intercept\n",
    "print (\"The Intercept of the model is : %f\" %intercept)\n",
    "\n",
    "#Evaluate the model using metric like Mean Absolute Error(MAE), Root Mean Square Error(RMSE) and R-Square\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluation = RegressionEvaluator(labelCol=\"Profit\", predictionCol=\"prediction\")\n",
    "\n",
    "# Root Mean Square Error\n",
    "rmse = evaluation.evaluate(pred.predictions, {evaluation.metricName: \"rmse\"})\n",
    "print(\"RMSE: %.3f\" % rmse)\n",
    "\n",
    "# Mean Square Error\n",
    "mse = evaluation.evaluate(pred.predictions, {evaluation.metricName: \"mse\"})\n",
    "print(\"MSE: %.3f\" % mse)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = evaluation.evaluate(pred.predictions, {evaluation.metricName: \"mae\"})\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# r2 - coefficient of determination\n",
    "r2 = evaluation.evaluate(pred.predictions, {evaluation.metricName: \"r2\"})\n",
    "print(\"r2: %.3f\" %r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f053018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321cc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3edf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f40606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952263c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
